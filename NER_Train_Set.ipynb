{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "max_sent_tokens = 75\n",
    "n_folds = 5\n",
    "\n",
    "seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import us\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from random import shuffle\n",
    "\n",
    "data_path = '/media/rbshaffer/My Paasport/Current_Materials/dissertation/Legislative_Data/Legislation/UnitedStates/Annual'\n",
    "\n",
    "file_list = os.listdir(data_path)\n",
    "\n",
    "sys.path.append('/home/rbshaffer/PycharmProjects/Legislative_Data/')\n",
    "from _country_entities_annual import UnitedStates\n",
    "\n",
    "blacklist = ['Guam', 'Federated States of Micronesia','American Samoa', \n",
    "             'Puerto Rico', 'Virgin Islands',  'Northern Mariana Islands', \n",
    "             'Republic of The Marshall Islands', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/rbshaffer/PycharmProjects/Legislative_Data/agency_list.txt', 'r') as f:\n",
    "    agency_list = f.read().split('\\n')\n",
    "    \n",
    "agency_list = [e for e in agency_list if e and e not in [s.name for s in us.states.STATES]]\n",
    "agency_list = [e for e in agency_list if e not in blacklist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_inds(inst_list, sentence_str, tokens_list, label, tag_set, label_list):\n",
    "    for inst_str in inst_list:\n",
    "        if inst_str + ' ' in sentence_str or ' ' + inst_str in sentence_str:\n",
    "            entity_len = len(inst_str.split())\n",
    "            match_inds = [i for i in range(len(tokens_list)) \n",
    "                          if tokens_list[i:i + entity_len] == inst_str.split()]\n",
    "            for i in match_inds:\n",
    "                if i not in tag_set:\n",
    "                    tag_set[i] = 'B-MISC'\n",
    "                    \n",
    "                    label_list.append(label)\n",
    "                if entity_len > 1:\n",
    "                    for j in range(i+1, i+entity_len):\n",
    "                        if j not in tag_set:\n",
    "                            tag_set[j] = 'I-MISC'\n",
    "    \n",
    "    return tag_set, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "fold_assignments = random.choice(range(n_folds), len(agency_list))\n",
    "\n",
    "train_sets = {i: {'names': [agency_list[j] for j in range(len(agency_list)) if fold_assignments[j] != i],\n",
    "                  'sents': []}\n",
    "              for i in range(n_folds)}\n",
    "\n",
    "test_sets = {i: {'names': [agency_list[j] for j in range(len(agency_list)) if fold_assignments[j] == i],\n",
    "                 'sents': []}\n",
    "              for i in range(n_folds)}\n",
    "\n",
    "full_set = []\n",
    "\n",
    "for file_name in file_list:\n",
    "    print(re.sub('_', '/', file_name).strip('.json'))\n",
    "\n",
    "    with open(data_path + '/' + file_name, 'r') as f:\n",
    "        content = json.loads(f.read())\n",
    "\n",
    "        if content['parsed']:\n",
    "            # imported from system path above\n",
    "            parser = UnitedStates(content['parsed'], load_lstm=False)\n",
    "            for chunk in parser.chunks:\n",
    "                sentences = sent_tokenize(chunk)\n",
    "                for sentence in sentences:\n",
    "                    sentence = re.sub('\\s+', ' ', sentence)\n",
    "                    tokens = word_tokenize(sentence)\n",
    "                    \n",
    "                    for i in range(n_folds):\n",
    "                        tags = {}\n",
    "                        labels = []\n",
    "                        \n",
    "                        tags, labels = process_inds(train_sets[i]['names'], sentence, tokens, 'train', tags, labels)\n",
    "                        tags, labels = process_inds(test_sets[i]['names'], sentence, tokens, 'test', tags, labels)     \n",
    "                    \n",
    "                        tags.update({j: 'O' for j in range(len(tokens)) if j not in tags.keys()})\n",
    "                        to_append = '\\n'.join(tokens[j] + ' ' + tags[j] for j in range(len(tokens)))\n",
    "\n",
    "                        if 'train' in labels and 'test' not in labels:\n",
    "                            train_sets[i]['sents'].append(to_append)\n",
    "                        elif 'test' in labels and 'train' not in labels:\n",
    "                            test_sets[i]['sents'].append(to_append)\n",
    "                            \n",
    "                    if 'test' in labels or 'train' in labels:\n",
    "                        full_set.append(to_append)      \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/rbshaffer/sequence_tagging/data/data_v2_full.txt', 'w') as f:\n",
    "    f.write('\\n\\n'.join([e for e in full_set if len(e.split()) < max_sent_tokens and 'MISC' in e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_folds):\n",
    "    train_list = [e for e in train_sets[i]['sents'] if len(e.split()) < max_sent_tokens and 'MISC' in e]\n",
    "    shuffle(train_list)\n",
    "\n",
    "    test_list = [e for e in test_sets[i]['sents'] if len(e.split()) < max_sent_tokens and 'MISC' in e]\n",
    "    shuffle(test_list)\n",
    "\n",
    "    with open('/home/rbshaffer/sequence_tagging/data/data_v2_' + str(i) + '_train.txt', 'w') as f:\n",
    "        f.write('\\n\\n'.join(train_list))\n",
    "\n",
    "    with open('/home/rbshaffer/sequence_tagging/data/data_v2_' + str(i) + '_test.txt', 'w') as f:\n",
    "        f.write('\\n\\n'.join(test_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
